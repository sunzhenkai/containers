version: "2"
networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: "172.50.0.0/24"
          gateway: "172.50.0.1"
volumes:
  data:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data
  kafka:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/kafka
  hadoop:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/hadoop
  mariadb:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/mariadb
  clickhouse:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/clickhouse
  consul:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/consul
  zookeeper:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/zookeeper
  postgresql:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/postgresql
  redis:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/redis
  mongodb:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/mongodb
  grafana:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/grafana
  dolphinscheduler-worker-data:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/dolphinscheduler-worker-data
  dolphinscheduler-logs:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/dolphinscheduler-logs
  dolphinscheduler-shared-local:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/dolphinscheduler-shared-local
  dolphinscheduler-resource-local:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/dolphinscheduler-resource-local
  flink:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: /data/docker/datascience/data/flink
services:
  hadoop-namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    restart: always
    ports:
      - 9870:9870
      - 8020:8020
      - 8020:8020/udp
    env_file:
      - ./config/hadoop/config
    volumes:
      - hadoop:/data
      # - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    environment:
      ENSURE_NAMENODE_DIR: "/data/dfs/name"
    networks:
      - default
  hadoop-datanode:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    restart: always
    env_file:
      - ./config/hadoop/config
    networks:
      - default
    volumes:
      - hadoop:/data
  hadoop-resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    restart: always
    ports:
      - 8088:8088
    env_file:
      - ./config/hadoop/config
    networks:
      - default
    volumes:
      - hadoop:/data
  hadoop-nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    restart: always
    env_file:
      - ./config/hadoop/config
    networks:
      - default
    volumes:
      - hadoop:/data
  development-node:
    image: sunzhenkai/development:0.0.3
    command: ["/sbin/init"]
    restart: always
    privileged: true
    networks:
      - default
    ports:
      - 2025:22
      - 8285:8085
    volumes:
      - data:/data
  spark:
    image: docker.io/bitnami/spark:3.4
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - 8286:8080
      - 8277:7077
    networks:
      - default
  spark-worker:
    image: docker.io/bitnami/spark:3.4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=60G
      - SPARK_WORKER_CORES=30
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks:
      - default
    deploy:
      mode: replicated
      replicas: 2
  zeeplin:
    image: apache/zeppelin:0.10.1
    environment:
      - ZEPPELIN_LOG_DIR='/data/zeeplin/logs'
      - ZEPPELIN_NOTEBOOK_DIR='/data/zeeplin/notebook'
    ports:
      - 8280:8080
    networks:
      - default
    volumes:
      - data:/data
  flink-jobmanager:
    image: docker.io/bitnami/flink:1
    ports:
      - 8223:6123
      - 8220:8081
    environment:
      - FLINK_MODE=jobmanager
      - FLINK_CFG_REST_BIND__ADDRESS=0.0.0.0
    networks:
      - default
    volumes:
      - ./config/flink/master:/bitnami/flink/conf/master
      - ./config/flink/flink-conf.yaml:/bitnami/flink/conf/flink-conf.yaml
      - ./config/flink/log4j-console.properties:/bitnami/flink/conf/log4j-console.properties
      - ./config/flink/log4j-session.properties:/bitnami/flink/conf/log4j-session.properties
      - ./config/flink/log4j.properties:/bitnami/flink/conf/log4j.properties
      - ./config/flink/logback-console.xml:/bitnami/flink/conf/logback-console.xml
      - ./config/flink/logback-session.xml:/bitnami/flink/conf/logback-session.xml
      - ./config/flink/logback.xml:/bitnami/flink/conf/logback.xml
      - ./config/flink/workers:/bitnami/flink/conf/workers
      - ./config/flink/zoo.cfg:/bitnami/flink/conf/zoo.cfg
  flink-taskmanager:
    image: docker.io/bitnami/flink:1
    user: root
    ports:
      - 8221:6121
      - 8222:6122
    environment:
      - FLINK_MODE=taskmanager
      - FLINK_JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    networks:
      - default
    volumes:
      - ./config/flink/master:/bitnami/flink/conf/master
      - ./config/flink/flink-conf.yaml:/bitnami/flink/conf/flink-conf.yaml
      - ./config/flink/log4j-console.properties:/bitnami/flink/conf/log4j-console.properties
      - ./config/flink/log4j-session.properties:/bitnami/flink/conf/log4j-session.properties
      - ./config/flink/log4j.properties:/bitnami/flink/conf/log4j.properties
      - ./config/flink/logback-console.xml:/bitnami/flink/conf/logback-console.xml
      - ./config/flink/logback-session.xml:/bitnami/flink/conf/logback-session.xml
      - ./config/flink/logback.xml:/bitnami/flink/conf/logback.xml
      - ./config/flink/workers:/bitnami/flink/conf/workers
      - ./config/flink/zoo.cfg:/bitnami/flink/conf/zoo.cfg
  # flink-taskmanager-b:
  #   image: docker.io/bitnami/flink:1
  #   ports:
  #     - 8225:6121
  #     - 8226:6122
  #   environment:
  #     - FLINK_MODE=taskmanager
  #     - FLINK_JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
  #   networks:
  #     - default
  #   volumes:
  #     - ./config/flink/master:/bitnami/flink/conf/master
  #     - ./config/flink/flink-conf.yaml:/bitnami/flink/conf/flink-conf.yaml
  kafka:
    image: docker.io/bitnami/kafka:3.5
    ports:
      - 8292:9092
    volumes:
      - "kafka:/bitnami"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    networks:
      - default
  consul:
    image: docker.io/bitnami/consul:1
    user: root
    volumes:
      - consul:/bitnami/consul
    ports:
      - '8300:8300'
      - '8301:8301'
      - '8301:8301/udp'
      - '8500:8500'
      - '8600:8600'
      - '8600:8600/udp'
    environment:
      - CONSUL_DISABLE_KEYRING_FILE=true
      - CONSUL_BIND_ADDR=127.0.0.1
    networks:
      - default
  clickhouse:
    image: docker.io/bitnami/clickhouse:23
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '8233:8123'
    volumes:
      - clickhouse:/bitnami/clickhouse
    networks:
      - default
  postgresql:
    user: root
    image: docker.io/bitnami/postgresql:15
    volumes:
      - 'postgresql:/bitnami/postgresql'
    environment:
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '5432:5432'
    healthcheck:
      test: ["CMD", "bash", "-c", "cat < /dev/null > /dev/tcp/127.0.0.1/5432"]
      interval: 5s
      timeout: 60s
      retries: 120
    networks:
      - default
  redis:
    image: docker.io/bitnami/redis:7.0
    volumes:
      - 'redis:/bitnami'
    environment:
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '6379:6379'
    networks:
      - default
  airflow-scheduler:
    image: docker.io/bitnami/airflow-scheduler:2
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
    networks:
      - default
  airflow-worker:
    image: docker.io/bitnami/airflow-worker:2
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
    networks:
      - default
  airflow:
    image: docker.io/bitnami/airflow:2
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_PASSWORD=bitnami123
      - AIRFLOW_USERNAME=user
      - AIRFLOW_EMAIL=user@example.com
    ports:
      - '8208:8080'
    networks:
      - default
  prometheus:
    image: docker.io/bitnami/prometheus:2
    ports:
      - '8290:9090'
    networks:
      - default
  mongodb:
    image: docker.io/bitnami/mongodb:6.0
    ports:
      - "27017:27017"
    volumes:
      - 'mongodb:/bitnami/mongodb'
    networks:
      - default
  mariadb:
    image: docker.io/bitnami/mariadb:10.9.7
    ports:
      - '3306:3306'
    volumes:
      - 'mariadb:/bitnami/mariadb'
    environment:
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    healthcheck:
      test: ['CMD', '/opt/bitnami/scripts/mariadb/healthcheck.sh']
      interval: 15s
      timeout: 5s
      retries: 6
    networks:
      - default
  phpmyadmin:
    image: docker.io/bitnami/phpmyadmin:5
    ports:
      - '8283:8080'
      - '8243:8443'
    depends_on:
      - mariadb
    networks:
      - default
  grafana:
    image: docker.io/bitnami/grafana:10
    ports:
      - '3000:3000'
    environment:
      - 'GF_SECURITY_ADMIN_PASSWORD=bitnami'
    volumes:
      - grafana:/opt/bitnami/grafana/data
    networks:
      - default
  zookeeper:
    image: docker.io/bitnami/zookeeper:3.8
    user: root
    ports:
      - '2181:2181'
    volumes:
      - 'zookeeper:/bitnami/zookeeper'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - default
    healthcheck:
      test: ["CMD", "bash", "-c", "cat < /dev/null > /dev/tcp/127.0.0.1/2181"]
      interval: 5s
      timeout: 60s
      retries: 120
      
  # dolphinscheduler
  dolphinscheduler-schema-initializer:
    image: apache/dolphinscheduler-tools:latest
    profiles: ["init"]
    command: [ tools/bin/upgrade-schema.sh ]
    env_file:
      - ./config/dolphinscheduler/env
    depends_on:
      postgresql:
        condition: service_healthy
    volumes:
      - dolphinscheduler-logs:/opt/dolphinscheduler/logs
      - dolphinscheduler-shared-local:/opt/soft
      - dolphinscheduler-resource-local:/dolphinscheduler
    networks:
      - default
  dolphinscheduler-api:
    image: apache/dolphinscheduler-api:latest
    env_file:
      - ./config/dolphinscheduler/env
    ports:
      - "12345:12345"
      - "25333:25333"
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:12345/dolphinscheduler/actuator/health" ]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      zookeeper:
        condition: service_healthy
    volumes:
      - dolphinscheduler-logs:/opt/dolphinscheduler/logs
      - dolphinscheduler-shared-local:/opt/soft
      - dolphinscheduler-resource-local:/dolphinscheduler
    networks:
      - default
  dolphinscheduler-alert:
    image: apache/dolphinscheduler-alert-server:latest
    env_file:
      - ./config/dolphinscheduler/env
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:50053/actuator/health" ]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      zookeeper:
        condition: service_healthy
    volumes:
      - dolphinscheduler-logs:/opt/dolphinscheduler/logs
    networks:
      - default
  dolphinscheduler-master:
    image: apache/dolphinscheduler-master:latest
    env_file:
      - ./config/dolphinscheduler/env
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:5679/actuator/health" ]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      zookeeper:
        condition: service_healthy
    volumes:
      - dolphinscheduler-logs:/opt/dolphinscheduler/logs
      - dolphinscheduler-shared-local:/opt/soft
    networks:
      - default
  dolphinscheduler-worker:
    image: apache/dolphinscheduler-worker:latest
    env_file:
      - ./config/dolphinscheduler/env
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:1235/actuator/health" ]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      zookeeper:
        condition: service_healthy
    volumes:
      - dolphinscheduler-worker-data:/tmp/dolphinscheduler
      - dolphinscheduler-logs:/opt/dolphinscheduler/logs
      - dolphinscheduler-shared-local:/opt/soft
      - dolphinscheduler-resource-local:/dolphinscheduler
    networks:
      - default
  azkaban-executor-server:
    image: sunzhenkai/azkaban-executor-server:839965af353368824435254f1b23b00e9937c24a
    restart: always
    environment:
      AZKABAN_MYSQL_HOST: mariadb
    networks:
      - default
  azkaban-web-server:
    image: sunzhenkai/azkaban-web-server:839965af353368824435254f1b23b00e9937c24a
    restart: always
    environment:
      AZKABAN_MYSQL_HOST: mariadb
    networks:
      - default
    ports:
      - 8261:8081
    depends_on:
      azkaban-executor-server:
        condition: service_healthy